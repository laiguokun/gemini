% Gemini theme
% https://github.com/anishathalye/gemini

\documentclass[final]{beamer}

% ====================
% Packages
% ====================

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[size=custom,width=120,height=72,scale=1.0]{beamerposter}
\usetheme{gemini}
\usecolortheme{gemini}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{pgfplots}

\input{symbol}
\input{custom}
\usepackage{tikzsymbols}
\usepackage{changepage}

% ====================
% Lengths
% ====================

% If you have N columns, choose \sepwidth and \colwidth such that
% (N+1)*\sepwidth + N*\colwidth = \paperwidth
\newlength{\sepwidth}
\newlength{\colwidth}
\setlength{\sepwidth}{0.025\paperwidth}
\setlength{\colwidth}{0.3\paperwidth}

\newcommand{\separatorcolumn}{\begin{column}{\sepwidth}\end{column}}
%DeclareTextCommandDefault{\nobreakspace}{\leavevmode\nobreak\ } 

% ====================
% Title
% ====================

\title{Re-examination of the Role of Latent Variables in Sequence Modeling}

\author{Guokun Lai \inst{1} \& Zihang Dai \inst{1} \& Yiming Yang \inst{1} \& Shinjae Yoo \inst{2}}

\institute[shortinst]{\inst{1} Carnegie Mellon University \samelineand \inst{2} Brookhaven National Laboratory}

% ====================
% Footer (optional)
% ====================

%\footercontent{
 % \href{https://www.example.com}{https://www.example.com} \hfill
 % ABC Conference 2025, New York --- XYZ-1234 \hfill
  %\href{mailto:alyssa.p.hacker@example.com}{alyssa.p.hacker@example.com}}
% (can be left out to remove footer)

% ====================
% Body
% ====================

\begin{document}

\begin{frame}[t]
\begin{columns}[t]
\separatorcolumn

\begin{column}{\colwidth}

  \begin{block}{Problem Overview}

	    \begin{itemize}
		\item \textbf{Task}: Density estimation of sequences, i.e. $p(\rvx)$
		\item \textbf{Problem}: Distinct behavior of \textbf{stochastic variables} in sequence modeling
	    \begin{itemize}
			\item \textbf{Multi-variate} setting: stochastic variables \magenta{\bf improve} log-likelihood (e.g. speech, midi music~\cite{chung2015recurrent,fraccaro2016sequential,goyal2017z})
			\item \textbf{Uni-variate} setting: stochastic variables \cyan{\bf harm} log-likelihood (e.g. language / image modeling)
		\end{itemize} 
		\item \textbf{Goal}: A better understanding of \textbf{why} or \textbf{whether} stochastic variables are helpful in the sequence modeling
		\item \textbf{Notation}: Denote the data sample as $\rvx = \{\rvx_1, \rvx_2, \cdots, \rvx_T\}, \rvx_\tau \in \R^L$
		\end{itemize}
		\vspace{-1em}
  \end{block}

  \begin{block}{Basic framework - RNN}
	\begin{figure}
		\includegraphics[width=0.85\linewidth]{fig/RNN-L4.pdf}
	\end{figure}
	\vspace{-1em}
	\begin{align*}
	\log p(\rvx) 
	= \sum_{\tau=1}^{T} \log p(\rvx_{\tau} \mid \rvx_{<\tau})
	\approx \sum_{\tau=1}^{T} \sum_{i=1}^{L} \log p(x_{\tau,i} \mid \rvx_{<\tau})
	\end{align*}
	\vspace{-1em}
  \end{block}

  \begin{block}{Basic framework - Stochastic RNN (SRNN) via VAE}
	\begin{figure}
	\includegraphics[width=0.85\linewidth]{fig/SRNN-L4-ELBO.pdf} % {fig/SRNN-L4-prior.pdf}
	\end{figure}
	\vspace{-1em}
	\begin{align*}
	\log p(\rvx) 
	&= \sum_{\tau=1}^{T} \log p(\rvx_{\tau} \mid \rvx_{<\tau}) 
	= \sum_{\tau=1}^{T} \log \int p(\rvx_{\tau}, \rvz_\tau \mid \rvx_{<\tau}) d\rvz_\tau && \text{(add $\rvz_\tau$ for each $\rvx_\tau$)} \\
	&= \sum_{\tau=1}^{T} \log \int p(\rvx_{\tau} \mid \rvz_\tau , \rvx_{<\tau}) p(\rvz_\tau \mid \rvx_{<\tau}) d\rvz_\tau && \text{(product rule)} \\
	&\approx \sum_{\tau=1}^{T} \log \int \sbr{ \prod_{i=1}^{L} p(x_{\tau,i} \mid \rvz_\tau , \rvx_{<\tau}) } p(\rvz_\tau \mid \rvx_{<\tau}) d\rvz_\tau && \text{(factorized)}
	\end{align*}
\end{block}
\end{column}

\separatorcolumn

\begin{column}{\colwidth}

%  \begin{block}{Optimization of Stochastic RNN}
%	\begin{figure}
%	\includegraphics[width=0.9\linewidth]{fig/SRNN-L4-ELBO.pdf}
%	\end{figure}
%  \end{block}

\begin{block}{Implicit Unfairness in Capacity}
 	\textbf{RNN}: \textbf{fully factorized} prediction distribution
 	\[ 
 		p^\text{RNN}(x_{\tau} \mid \rvx_{<\tau}) \approx \prod_{i=1}^{L} p(x_{\tau,i} \mid \rvx_{<\tau})
 	\]
 	\textbf{SRNN}: \textbf{NOT} fully factorized anymore due to the integration
 	\[
 		p^\text{SRNN}(x_{\tau} \mid \rvx_{<\tau}) \approx \int  \prod_{i=1}^{L} p(x_{\tau,i} \mid \rvx_{<\tau}, \rvz_\tau) p(\rvz_\tau \mid \rvx_{< \tau}) d \rvz_\tau
 	\]
 	
 	\textbf{In summary},
 	\begin{itemize}
 	\item SRNN is able to model the correlation within each step $\rvx_\tau$, i.e. $\set{x_{\tau,i}}_{i=1}^{L}$
 	\item RNN does not have the capacity due to the fully factorized design
 	\end{itemize}
\end{block}

\begin{block}{How does SRNN model the intra-step correlation?}
	Examine the ELBO for a prediction step $\tau$:
	\begin{equation}\label{eqn:elbo}
		\mc{L}^\text{SRNN}_\tau = \E_{\rvz_\tau \sim q(\rvz_{\tau} \mid \rvx)} \sbr{ \sum_{i=1}^{L} \log p(x_{\tau,i} \mid \rvz_\tau, \rvx_{<\tau}) } - \text{KL}\rbr{q(\rvz_{\tau} \mid \rvx) \| p(\rvz_{\tau} \mid \rvx_{<\tau})}
	\end{equation}
	\textbf{Observation}: $\rvz_{\tau}$ conditions on $\rvx_\tau$ ($\rvx$ includes $\rvx_\tau$) $\implies$ SRNN can \textbf{leak ``\cyan{partial information}''} of $\rvx_{\tau}$ through $\rvz_{\tau}$
	\begin{itemize}
		\item \textbf{Recon. term}: the cost of predicting the rest of the information \textbf{given the ``\cyan{partial information}''}
		\item \textbf{KL term}: the cost of \textbf{obtaining the ``\cyan{partial information}''}
	\end{itemize} 
\end{block}

\begin{alertblock}{A concrete example}
 	(I) Assume $\rvz_{\tau} = x_{\tau, L} \in \R$, i.e., leak the last element of $\rvx_{\tau}$ as the partial information. Formally,
 	\[ q(\rvz_{\tau} \mid \rvx) = \delta_{\rvz_{\tau} = x_{\tau, L}} = 
 	\begin{cases}
 	\infty, & \text{if $\rvz_{\tau} = x_{\tau, L}$} \\
 	0, &\text{if $\rvz_{\tau} \neq x_{\tau, L}$} \\
 	\end{cases} 
 	\]
 	
 	(II) Plug this term into ELBO equation \eqref{eqn:elbo}:
 	\begin{align*}
 	\mc{L}^\text{SRNN}_\tau
 	&= \cancel{ \gray{ \sbr[\bigg]{ \log p(x_{\tau,L} \mid x_{\tau,L}, \rvx_{<\tau}) - \log q(x_{\tau,L} \mid \rvx) } } } && \text{$\implies$ cancel out} \\
 	&+ \sbr[\bigg]{ 
 		\underbrace{\vphantom{\sum_{i=1}^{L-1}} \log p(x_{\tau,L} \mid \rvx_{<\tau})}_\text{Predict $x_{\tau,L}$ first} + 
 		\underbrace{\sum_{i=1}^{L-1} \log p(x_{\tau,i} \mid x_{\tau,L}, \rvx_{<\tau}) }_\text{Predict others conditioned on $x_{\tau,L}$}}
 	\end{align*}
 	
 	(III) Compare SRNN with RNN:
 	\begin{align*}
 	\mc{L}^\text{SRNN}_\tau
 	&= \log p(x_{\tau,L} \mid \rvx_{<\tau}) + \sum_{i=1}^{L-1} \log p(x_{\tau,i} \mid \magenta{x_{\tau,L}}, \rvx_{<\tau}) \\
 	\mc{L}^\text{RNN}_\tau
 	%  &= \sum_{i=1}^{L} \log p(s_{\tau,i} \mid \rvs_{<\tau}) \\ 
 	&= \log p(x_{\tau,L} \mid \rvx_{<\tau}) + \sum_{i=1}^{L-1} \log p(x_{\tau,i} \mid \rvx_{<\tau})
 	\end{align*}
 	\begin{itemize}
 		\item Addition intra-step dependency \magenta{\bf shown in SRNN} that is \cyan{\bf absent in RNN}
 	\end{itemize}
\end{alertblock}

\textbf{Question}: what if we allow RNN to model intra-step dependency?

\end{column}

\separatorcolumn

\begin{column}{\colwidth}

\begin{block}{Simulate information leak with RNN}
	(I) Consider a more general case:
	\begin{itemize}
	\item Leak a subset $\rvc_\tau \subset \rvx_\tau$ of $\rvx_\tau$
	\item Predict the rest $\rvx_\tau \backslash \rvc_\tau$ given $\rvc_\tau$
	\end{itemize}
	  
	(II) Simulate the case in RNN with the following objective function:
	\begin{align*}
		\log p(\rvx_{\tau} \mid \rvx_{<\tau}) \approx \log p(\rvc_{\tau} \mid \rvx_{<\tau}) + \log p(\rvx_{\tau} \backslash \rvc_{\tau} \mid \rvx_{<\tau}, \rvc_{\tau})
	\end{align*}
	
	(III) Two strategies to construct the subset $\rvc_{\tau}$:
	\begin{itemize}
		\item Interleave, every $U$ elements: $\rvc_{\tau} = \seq{x_{\tau,1}, x_{\tau,U+1}, x_{\tau,2U+1}, \cdots }$
		\item Randomly choose $V$ distinct elements : $\rvc_{\tau} = \text{random}(\rvx_{\tau}, V)$
	\end{itemize}

	\begin{table}[!h]
		\small
		\centering
		\begin{tabular}{l|c c c}
			\toprule
			\bf Models               & \bf TIMIT  & \bf VCTK  & \bf Blizzard \\
			\midrule
			RNN                       & 32,745     & 0.786     & 7,610  \\
			SRNN                      & 69,296       & \bf 2.383 & 15,258 \\
			\midrule
			$\delta$-RNN ($U = 2$)   & 70,900     & 2.027     & \bf 15,306 \\
			$\delta$-RNN ($U = 3$)   & \bf 72,067 & 2.262     & 15,284 \\
			\midrule
			$\delta$-RNN ($V = 50$)  & 66,122     & 2.199     & 14,389 \\
			$\delta$-RNN ($V = 75$)  & 66,453     & 2.120     & 14,585 \\
			\bottomrule
		\end{tabular}
		\caption{Performance comparison between $\delta$-RNN and SRNN. Note that a smaller $U$ corresponds to leaking more elements. In these experiments, $L=200$}
		\vspace{-1.5em}
		\label{tab:inter_step_correlation}
	\end{table}
\end{block}

\begin{block}{Modeling Intra-step Correlation with an Auto-Regressive Structure}
	Introduce the auto-regressive structure in both RNN \& SRNN:
	\begin{align}
	\label{eqn:ar_rnn_out_dist}
	p_\theta^\text{RNN-hier}(x_t \mid \rvx_{<t})
	&= \prod_{i=1}^{L} p_{\theta}(x_{t,i} \mid \rvx_{<t}, x_{t,<i}), \\
	\label{eqn:ar_srnn_out_dist}
	p_\theta^\text{SRNN-hier}(x_t \mid \rvz_{\le t}, \rvx_{<t})
	&= \prod_{i=1}^{L} p_{\theta}(x_{t,i} \mid \rvz_{\le t}, \rvx_{<t}, x_{t,<i}).
	\end{align}
	\begin{table}[!t]
		\centering
		\small
		\begin{tabular}{l|c c c | c c }
			\toprule
			\bf Models          & \bf TIMIT & \bf VCTK  & \bf Blizzard  & \bf Muse  & \bf Nottingham \\
			\midrule
			VRNN~\cite{chung2015recurrent}      & 28,982      & -          & 9,392      &  -         &  -      \\
			SRNN~\cite{fraccaro2016sequential}      & 60,550      & -          & 11,991     &  -6.28     &  -2.94     \\\
			Z-Forcing~\cite{goyal2017z}  & 70,469 & -          & 15,430     &  -         &  -       \\
			\midrule
			RNN               & 32,745      & 0.786      & 7,610     & -6.991     & -3.400     \\
			\midrule\midrule
			RNN-hier            & \textbf{109,641}     & \textbf{3.1822}     & \textbf{21,950}     & \textbf{-5.161} & \textbf{-2.028} \\
			SRNN-hier           & 107,912     & 3.1423     & 21,845     & -5.483     & -2.065    \\
			\bottomrule
		\end{tabular}
		\caption{Performance comparison on a diverse set of datasets. - suggests the model is not application on the dataset. }
		\label{tab:non_factorized}
		\vspace{-1.5em}
	\end{table}
	\vspace{-1.0em}
\end{block}

\begin{block}{Conclusion}
	(1) Latent variables provides a mechanism to leverage the intra-step correlation
	
	(2) However, its capacity is clearly weaker compared to a simple auto-regressive structure
%  	From our empirical observation, the main effect of latent variables is only to provide a mechanism to leverage the intra-step correlation, which is however, not as powerful as employing the straightforward auto-regressive decomposition.
%  	It remains unclear what leads to the significant gap between the theoretical potential of latent variables and their practical effectiveness, which we believe deserves more research attention.
%  	\vspace{-1.0em}
\end{block}

\begin{block}{References}
	\vspace{-0.2em}
    \nocite{*}
	\setbeamertemplate{bibliography entry article}{}
	\setbeamertemplate{bibliography entry title}{}
	\setbeamertemplate{bibliography entry location}{}
	\setbeamertemplate{bibliography entry note}{}
    \footnotesize{\bibliographystyle{plain}\bibliography{poster}}
\end{block}

\end{column}

\separatorcolumn
\end{columns}
\end{frame}

\end{document}
