% Gemini theme
% https://github.com/anishathalye/gemini

\documentclass[final]{beamer}

% ====================
% Packages
% ====================

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[size=custom,width=120,height=72,scale=1.0]{beamerposter}
\usetheme{gemini}
\usecolortheme{gemini}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{tikz}
\usepackage{pgfplots}

% ====================
% Lengths
% ====================

% If you have N columns, choose \sepwidth and \colwidth such that
% (N+1)*\sepwidth + N*\colwidth = \paperwidth
\newlength{\sepwidth}
\newlength{\colwidth}
\setlength{\sepwidth}{0.025\paperwidth}
\setlength{\colwidth}{0.3\paperwidth}

\newcommand{\separatorcolumn}{\begin{column}{\sepwidth}\end{column}}
%DeclareTextCommandDefault{\nobreakspace}{\leavevmode\nobreak\ } 
\input{symbol}
\input{custom}
% ====================
% Title
% ====================

\title{Re-examination of the Role of Latent Variables in Sequence Modeling}

\author{Guokun Lai \inst{1} \& Zihang Dai \inst{1} \& Yiming Yang \inst{1} \& Shinjae Yoo \inst{2}}

\institute[shortinst]{\inst{1} Carnegie Mellon University \samelineand \inst{2} Brookhaven National Laboratory}

% ====================
% Footer (optional)
% ====================

%\footercontent{
 % \href{https://www.example.com}{https://www.example.com} \hfill
 % ABC Conference 2025, New York --- XYZ-1234 \hfill
  %\href{mailto:alyssa.p.hacker@example.com}{alyssa.p.hacker@example.com}}
% (can be left out to remove footer)

% ====================
% Body
% ====================

\begin{document}

\begin{frame}[t]
\begin{columns}[t]
\separatorcolumn

\begin{column}{\colwidth}

  \begin{block}{Problem Definition}

	    \begin{itemize}
		\item \textbf{Task}: Estimate the probability density of a data sample with a sequential structure, such as music scores and signal value of a detector cluster. 
		\item \textbf{Problem}: Previous publications show contradict conclusions about introducing stochastic variables into the sequential model (e.g. RNN) in multivariate and uni-variate domains. 
	    \begin{itemize}
			\item If testing on multivariate problems, adding stochastic variable would improve log-likelihood results \cite{chung2015recurrent,fraccaro2016sequential,goyal2017z}.
			\item If testing on uni-variate problems, such as language modeling and image density estimation, adding stochastic variable would harm log-likelihood results.
		\end{itemize} 
		\item \textbf{Goal}: To find out the true reason for distinct conclusions, and have a better understanding of the role of stochastic latent variables in the sequential model. 
		\item Notation: Denote the data sample as $x = \{\rvs_1, \rvs_2, \cdots, \rvs_T\}, \rvs \in \R^L $
		\end{itemize}
		\vspace{-1em}
  \end{block}

  \begin{block}{Basic framework - RNN}
	\begin{figure}
		\includegraphics[width=0.9\linewidth]{fig/RNN-L4.pdf}
	\end{figure}
	\vspace{-1em}
	\begin{align*}
	\log p(\rvx) 
	= \sum_{\tau=1}^{T} \log p(\rvs_{\tau} \mid \rvs_{<\tau})
	\end{align*}
	\vspace{-1em}
  \end{block}

  \begin{block}{Basic framework - Stochastic RNN}
	\begin{figure}
	\includegraphics[width=0.9\linewidth]{fig/SRNN-L4-prior.pdf}
	\end{figure}
	\vspace{-1em}
	\begin{align*}
	\log p(\rvx) 
	&= \sum_{\tau=1}^{T} \log p(\rvs_{\tau} \mid \rvs_{<\tau}) 
	= \sum_{\tau=1}^{T} \log \int p(\rvs_{\tau}, \rvz_\tau \mid \rvs_{<\tau}) d\rvz_\tau && \text{(add $\rvz_\tau$ for each $\rvs_\tau$)} \\
	&= \sum_{\tau=1}^{T} \log \int p(\rvs_{\tau} \mid \rvz_\tau , \rvs_{<\tau}) p(\rvz_\tau \mid \rvs_{<\tau}) d\rvz_\tau && \text{(product rule)} \\
	\end{align*}
\end{block}
\end{column}

\separatorcolumn

\begin{column}{\colwidth}

  \begin{block}{Optimization of Stochastic RNN}
	\begin{figure}
	\includegraphics[width=0.9\linewidth]{fig/SRNN-L4-ELBO.pdf}
	\end{figure}
  \end{block}

  \begin{block}{Revisiting the superior performance of SRNN}
		\textbf{Hypothesis}: SRNN can ``unfairly'' take advantage of the \magenta{segment correlation} within the output step
		\textbf{Informal argument}:
		\begin{itemize}
			\item Consider the loss for the following prediction step
			\[ \E_{\rvz_\tau \sim q(\rvz_{\tau} \mid \rvx)} \sbr{ \sum_{i=1}^{L} \log p(s_{\tau,i} \mid \rvz_\tau, \rvs_{<\tau}) } - \text{KL}\rbr{q(\rvz_{\tau} \mid \rvx) \| p(\rvz_{\tau} \mid \rvs_{<\tau})} \]
			\item $\rvz_{\tau}$ depends on $\rvx$ (including $\rvs_\tau$) $\implies$ $\rvz_{\tau}$ can contain ``\cyan{partial information}'' about $\rvs_{\tau}$
			\item KL term: the cost of obtaining the partial information 
			\item Reconstruction term: the cost of predicting the rest of the information \textbf{given the partial information}
		\item A concrete case: $\rvz_{\tau} \in \R$ and $\rvz_{\tau}$ contains the information of the element $s_{\tau, L}$ (the last element) in $\rvs_{\tau}$ (partial info.):
		\[ q(\rvz_{\tau} \mid \rvx) = \delta_{\rvz_{\tau} = s_{\tau, L}} = 
		\begin{cases}
		\infty, & \text{if $\rvz_{\tau} = s_{\tau, L}$} \\
		0, &\text{if $\rvz_{\tau} \neq s_{\tau, L}$} \\
		\end{cases} 
		\]
		
		\item Replace this term into ELBO loss function
		\begin{align*}
		\mc{L}_\text{SRNN} 
		&= \sbr[\bigg]{ \log p(s_{\tau,L} \mid s_{\tau,L}, \rvs_{<\tau}) - \log q(s_{\tau,L} \mid \rvx) } && \text{$\implies$ cancel out} \\
		&+ \sbr[\bigg]{ 
			\underbrace{\vphantom{\sum_{i=1}^{L-1}} \log p(s_{\tau,L} \mid \rvs_{<\tau})}_\text{Predict $s_{\tau,L}$ first} + 
			\underbrace{\sum_{i=1}^{L-1} \log p(s_{\tau,i} \mid s_{\tau,L}, \rvs_{<\tau}) }_\text{Predict others conditioned on $s_{\tau,L}$}}
		\end{align*}
		\item Compare SRNN with RNN:
		\begin{align*}
		\mc{L}_\text{SRNN} 
		&= \log p(s_{\tau,L} \mid \rvs_{<\tau}) + \sum_{i=1}^{L-1} \log p(s_{\tau,i} \mid \magenta{s_{\tau,L}}, \rvs_{<\tau}) \\
		\mc{L}_\text{RNN} 
		&= \sum_{i=1}^{L} \log p(s_{\tau,i} \mid \rvs_{<\tau}) \\ 
		&= \log p(s_{\tau,L} \mid \rvs_{<\tau}) + \sum_{i=1}^{L-1} \log p(s_{\tau,i} \mid \rvs_{<\tau}) \\
		\end{align*}
	\end{itemize}
  \end{block}

\end{column}

\separatorcolumn

\begin{column}{\colwidth}

  \begin{block}{Verify the Hypothesis via Experiments}
	In order to verify the hypothesis, we train a RNN with the loss function: 
	\begin{align*}
		\log p(\rvs_{\tau} \mid \rvs_{<\tau}) = \log p(\rvc_{\tau} \mid \rvs_{<\tau}) + \log p(\rvs_{\tau} \backslash \rvc_{\tau} \mid \rvs_{<\tau}, \rvc_{\tau})
	\end{align*}
	We explore two strategies to construct the subset $\rvc_{\tau}$.
	\begin{itemize}
		\item Interleave, every $U$ elements: $\rvc_{\tau} = \seq{s_{\tau,1}, s_{\tau,U+1}, s_{\tau,2U+1}, \cdots }$
		\item Randomly choose $V$ distinct elements : $\rvc_{\tau} = \text{random}(\rvs_{\tau}, V)$
	\end{itemize}

	\begin{table}[!h]
		\centering
		\begin{tabular}{l|c c c}
			\toprule
			\bf Models               & \bf TIMIT  & \bf VCTK  & \bf Blizzard \\
			\midrule
			RNN                       & 32,745     & 0.786     & 7,610  \\
			SRNN                      & 69,296       & \bf 2.383 & 15,258 \\
			\midrule
			$\delta$-RNN ($U = 2$)   & 70,900     & 2.027     & \bf 15,306 \\
			$\delta$-RNN ($U = 3$)   & \bf 72,067 & 2.262     & 15,284 \\
			\midrule
			$\delta$-RNN ($V = 50$)  & 66,122     & 2.199     & 14,389 \\
			$\delta$-RNN ($V = 75$)  & 66,453     & 2.120     & 14,585 \\
			\bottomrule
		\end{tabular}
		\caption{Performance comparison between $\delta$-RNN and SRNN. Note that a smaller $U$ corresponds to leaking more elements. In these experiments, $L=200$}
		\vspace{-1.5em}
		\label{tab:inter_step_correlation}
	\end{table}
  \end{block}

  \begin{block}{Modeling Simultaneity with Auto-Regressive Decomposition}
	We can explicitly introduce the intra-step dependency of variables to the sequential models (both RNN and SRNN) by using an auto-regressive decomposition at the intra-step level. More specifically, the RNN and SRNN output distribution are written as,
	\begin{align}
	\label{eqn:ar_rnn_out_dist}
	p_\theta(x_t \mid \rvx_{<t})
	&= \prod_{i=1}^{L} p_{\theta}(x_{t,i} \mid \rvx_{<t}, x_{t,<i}), \\
	\label{eqn:ar_srnn_out_dist}
	p_\theta(x_t \mid \rvz_{\le t}, \rvx_{<t})
	&= \prod_{i=1}^{L} p_{\theta}(x_{t,i} \mid \rvz_{\le t}, \rvx_{<t}, x_{t,<i}).
	\end{align}
	We named these models as RNN-hier and SRNN-hier. The experiment results are as follows,
	\begin{table}[!t]
		\centering
		\small
			\begin{tabular}{l|c c c | c c }
				\toprule
				\bf Models          & \bf TIMIT & \bf VCTK  & \bf Blizzard  & \bf Muse  & \bf Nottingham \\
				\midrule
				VRNN$^\dagger$\cite{chung2015recurrent}      & 28,982      & -          & 9,392      &  -         &  -      \\
				SRNN$^\dagger$\cite{fraccaro2016sequential}      & 60,550      & -          & 11,991     &  -6.28     &  -2.94     \\\
				Z-Forcing$^\dagger$\cite{goyal2017z}  & 70,469 & -          & 15,430     &  -         &  -       \\
				\midrule
				RNN               & 32,745      & 0.786      & 7,610     & -6.991     & -3.400     \\
				\midrule\midrule
				RNN-hier            & \bf 109,641     & \bf 3.1822     & \bf 21,950     & \textbf{-5.161} & \bf -2.028 \\
				SRNN-hier           & 107,912     & 3.1423     & 21,845     & -5.483     & -2.065    \\
				\bottomrule
			\end{tabular}
		\caption{Performance comparison on a diverse set of datasets. The models with $^\dagger$ indicate that the performances are directly copied from previous publications. N/A suggests the model is not application on the dataset. }
		\label{tab:non_factorized}
		\vspace{-1.5em}
	\end{table}
	\vspace{-1.0em}
  \end{block}
  \begin{block}{Conclusion}
  	From our empirical observation, the main effect of latent variables is only to provide a mechanism to leverage the intra-step correlation, which is however, not as powerful as employing the straightforward auto-regressive decomposition.
  	It remains unclear what leads to the significant gap between the theoretical potential of latent variables and their practical effectiveness, which we believe deserves more research attention.
  	\vspace{-1.0em}
  \end{block}
  \begin{block}{References}
    \nocite{*}
	\setbeamertemplate{bibliography entry article}{}
	\setbeamertemplate{bibliography entry title}{}
	\setbeamertemplate{bibliography entry location}{}
	\setbeamertemplate{bibliography entry note}{}
    \footnotesize{\bibliographystyle{plain}\bibliography{poster}}
  \end{block}

\end{column}

\separatorcolumn
\end{columns}
\end{frame}

\end{document}
